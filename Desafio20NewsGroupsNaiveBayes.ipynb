{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZjj5ouyKMAYPVqtu3rX+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-e-leiva/procesamientoDelHabla/blob/main/Desafio20NewsGroupsNaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups**\n",
        "\n",
        "## **Consignas:**\n",
        "\n",
        "**1**. **Vectorizar documentos**. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
        "\n",
        "\n",
        "**2**. **Entrenar modelos de clasificación Naïve Bayes** para maximizar el desempeño de clasificación\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB.\n",
        "\n",
        "**3**. **Transponer la matriz documento-término**. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
      ],
      "metadata": {
        "id": "xTVHdXD3aTVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rnfH4yEqa9Gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Vectorizar documentos y medir similaridad"
      ],
      "metadata": {
        "id": "h7-BMYp9F-oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Cargar el dataset 20 newsgroups (subset train)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Instanciar el vectorizador TF-IDF\n",
        "tfidfvect = TfidfVectorizer()\n",
        "\n",
        "# Fit-transformar los datos de entrenamiento\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Obtener el vocabulario\n",
        "vocabulary = tfidfvect.vocabulary_\n",
        "\n",
        "# Mostrar algunas palabras del vocabulario\n",
        "words = list(vocabulary.keys())\n",
        "print(f'Número total de palabras en el vocabulario: {len(words)}')\n",
        "print('Algunas palabras en el vocabulario:', words[:30])  # Muestra las primeras 10 palabras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDLQnryFJSCU",
        "outputId": "2428f7e4-2418-4dfe-f8ec-3497607d7043"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de palabras en el vocabulario: 101631\n",
            "Algunas palabras en el vocabulario: ['was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'door', 'sports', 'looked', 'to', 'be', 'from', 'late', '60s', 'early', '70s', 'called', 'bricklin', 'doors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeYXAS2aF80J",
        "outputId": "fe995720-5bb7-4d60-ce81-1f6f64a62603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento 8917:\n",
            "\n",
            "I am running Windows 3.1, Windows for Work Groups and just loaded Dos 6.\n",
            "\n",
            "What`s happening appears a \n",
            "\n",
            "Clase del documento original: comp.os.ms-windows.misc\n",
            "\n",
            "Documento similar 2140:\n",
            "Clase: comp.os.ms-windows.misc\n",
            "Contenido (primeros 300 caracteres): In comp.os.ms-windows.misc you write:\n",
            "\n",
            "\n",
            "you might want to look in windows FAQ for this one, but here\n",
            "\n",
            "Documento similar 3451:\n",
            "Clase: comp.os.ms-windows.misc\n",
            "Contenido (primeros 300 caracteres): Hi there,\n",
            "I'm having a bizarre video problem within Windows 3.1.  I have a 286 with\n",
            "a GVGA-16 video \n",
            "\n",
            "Documento similar 8907:\n",
            "Clase: comp.os.ms-windows.misc\n",
            "Contenido (primeros 300 caracteres): Thanks to all those people who recommended Workspace managers for\n",
            "Windows 3.1.  I found 3 shareware \n",
            "\n",
            "Documento similar 2391:\n",
            "Clase: comp.os.ms-windows.misc\n",
            "Contenido (primeros 300 caracteres): I quit windows normally to run a special DOS app, got done with it\n",
            "and tried to start windows.  Ok g\n",
            "\n",
            "Documento similar 8832:\n",
            "Clase: comp.windows.x\n",
            "Contenido (primeros 300 caracteres): Has anyone found a fix for the following problem?\n",
            "\n",
            "Client Software:\tSunOs 4.1.1, X11R5\n",
            "Server Hardwa\n",
            "\n",
            "================================================================================\n",
            "Documento 294:\n",
            "\n",
            "\n",
            "\n",
            "CNN just claimed he bought 104 \"semi-automatic assault rifles\".  And\n",
            "they say Koresh wasn't god-li \n",
            "\n",
            "Clase del documento original: talk.politics.guns\n",
            "\n",
            "Documento similar 5826:\n",
            "Clase: soc.religion.christian\n",
            "Contenido (primeros 300 caracteres): A listmember (D Andrew Killie, I think) wrote, in response to the\n",
            "suggestion that genocide may somet\n",
            "\n",
            "Documento similar 649:\n",
            "Clase: talk.religion.misc\n",
            "Contenido (primeros 300 caracteres): I told some friends of mine two weeks ago that Koresh was dead.  The FBI and\n",
            "the BATF could not let \n",
            "\n",
            "Documento similar 7144:\n",
            "Clase: talk.politics.guns\n",
            "Contenido (primeros 300 caracteres): \n",
            "\n",
            "Title 18, 241 and/or 242 seem to apply.  241 is conspiracy (two or\n",
            "more persons) against rights of\n",
            "\n",
            "Documento similar 7963:\n",
            "Clase: soc.religion.christian\n",
            "Contenido (primeros 300 caracteres): \n",
            "[ -and many others mailed me.  Here is a reply to one of the letters.  Seems to\n",
            "me that atheist do \n",
            "\n",
            "Documento similar 6894:\n",
            "Clase: talk.politics.guns\n",
            "Contenido (primeros 300 caracteres): Here is a press release from the White House.\n",
            "\n",
            " President Clinton's Remarks On Waco With Q/A\n",
            " To: Na\n",
            "\n",
            "================================================================================\n",
            "Documento 6354:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I meant to comment on this at the time.\n",
            "\n",
            "There's just no way baserunning could be that importa \n",
            "\n",
            "Clase del documento original: rec.sport.baseball\n",
            "\n",
            "Documento similar 10390:\n",
            "Clase: rec.sport.baseball\n",
            "Contenido (primeros 300 caracteres): \n",
            "\n",
            "\n",
            "\n",
            "I wouldn't give baserunning that much value.\n",
            "\n",
            "The above effect is clear, but there are other eff\n",
            "\n",
            "Documento similar 1485:\n",
            "Clase: rec.sport.baseball\n",
            "Contenido (primeros 300 caracteres): \n",
            "This was (my opinion) the stupidest thing in the Hidden Game. The\n",
            "argument was\n",
            "\n",
            "1) Defense, or runs\n",
            "\n",
            "Documento similar 724:\n",
            "Clase: rec.sport.baseball\n",
            "Contenido (primeros 300 caracteres): \n",
            "Well, actually, most of ours is based on what really happened and yours is\n",
            "based on some fantasy of\n",
            "\n",
            "Documento similar 10817:\n",
            "Clase: rec.sport.baseball\n",
            "Contenido (primeros 300 caracteres): \n",
            "I agree and disagree.  John is saying that the batters efforts will result\n",
            "in 4 more wins then loss\n",
            "\n",
            "Documento similar 7724:\n",
            "Clase: rec.sport.baseball\n",
            "Contenido (primeros 300 caracteres): : >Every single piece of evidence we can find points to Major League Baseball\n",
            ": >being 50% offense, \n",
            "\n",
            "================================================================================\n",
            "Documento 10926:\n",
            "\n",
            "Kent:\n",
            "\n",
            "     You say that\n",
            "\n",
            "                      ^^^^^^^^^\n",
            "                        Please don't!  ^^^ \n",
            "\n",
            "Clase del documento original: talk.religion.misc\n",
            "\n",
            "Documento similar 10076:\n",
            "Clase: talk.religion.misc\n",
            "Contenido (primeros 300 caracteres): Kent: With all due respect, how can I take you seriously, when you have\n",
            "the NAMES wrong in the 1st p\n",
            "\n",
            "Documento similar 9340:\n",
            "Clase: talk.religion.misc\n",
            "Contenido (primeros 300 caracteres): \n",
            "Can we assume from this statement that you are >unequivocally< saying that\n",
            "AMORC is not a spin off \n",
            "\n",
            "Documento similar 4428:\n",
            "Clase: talk.religion.misc\n",
            "Contenido (primeros 300 caracteres): ON the subject of how many competing RC orders there are, let me point out the\n",
            "Golden Dawn is only t\n",
            "\n",
            "Documento similar 1359:\n",
            "Clase: talk.religion.misc\n",
            "Contenido (primeros 300 caracteres): \n",
            "\n",
            "\tUp to that point I thought you were talking about the\n",
            "Rosicrucian Order... :-)  [No offense inten\n",
            "\n",
            "Documento similar 10344:\n",
            "Clase: sci.space\n",
            "Contenido (primeros 300 caracteres): ETHER IMPLODES 2 EARTH CORE, IS GRAVITY!!!\n",
            "\n",
            "  This paper BOTH describes how heavenly bodys can be st\n",
            "\n",
            "================================================================================\n",
            "Documento 961:\n",
            "\n",
            "\n",
            "Christian anti-Semitism comes from the obvious fact that the Jews should\n",
            "know the Hebrew Scriptures \n",
            "\n",
            "Clase del documento original: talk.religion.misc\n",
            "\n",
            "Documento similar 4805:\n",
            "Clase: talk.religion.misc\n",
            "Contenido (primeros 300 caracteres): \n",
            "Where does it say in the bible that Christians are supposed to persecute\n",
            "Jews?  Isn't it love your \n",
            "\n",
            "Documento similar 3216:\n",
            "Clase: soc.religion.christian\n",
            "Contenido (primeros 300 caracteres): [DISCLAIMER: Throughout this post, there are statements and questions which\n",
            "could easily be interpre\n",
            "\n",
            "Documento similar 1499:\n",
            "Clase: talk.religion.misc\n",
            "Contenido (primeros 300 caracteres): \n",
            "Do you consider Neo-Nazis and white supremists to be Christian?  I'd hardly\n",
            "classify them as Christ\n",
            "\n",
            "Documento similar 8456:\n",
            "Clase: alt.atheism\n",
            "Contenido (primeros 300 caracteres): \n",
            "That's not true.    I gave you two examples.   One was the rather\n",
            "pevasive anti-semitism in German \n",
            "\n",
            "Documento similar 507:\n",
            "Clase: talk.politics.mideast\n",
            "Contenido (primeros 300 caracteres):   I have never seen such immaturity among semitophiles.  This\n",
            "Andi Beyer character shows no signs of\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "#Vectorización de documentos\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el dataset 20 newsgroups (subset train)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Instanciar el vectorizador TF-IDF\n",
        "tfidfvect = TfidfVectorizer()\n",
        "\n",
        "# Fit-transformar los datos de entrenamiento\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Guardar las etiquetas de clasificación\n",
        "y_train = newsgroups_train.target\n",
        "\n",
        "#Tomamos 5 documentos al azar\n",
        "# Definimos un array con 5 índices de documentos aleatorios\n",
        "np.random.seed(111)  # Para reproducibilidad\n",
        "random_docs = np.random.choice(X_train.shape[0], 5, replace=False)\n",
        "\n",
        "#Medir similaridad con el resto de los documentos\n",
        "\n",
        "for idx in random_docs:\n",
        "    print(f\"Documento {idx}:\\n\")\n",
        "\n",
        "    # Mostrar el contenido del documento (solo los primeros 100 caracteres para tener una idea)\n",
        "    print(newsgroups_train.data[idx][:100], \"\\n\")\n",
        "\n",
        "    # Calcular la similaridad coseno entre el documento seleccionado y todos los demás\n",
        "    cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
        "\n",
        "    # Obtener los índices de los 5 documentos más similares (excluyendo el propio)\n",
        "    mostsim = np.argsort(cossim)[::-1][1:6]\n",
        "\n",
        "    # Mostrar la clase del documento original\n",
        "    print(f\"Clase del documento original: {newsgroups_train.target_names[y_train[idx]]}\\n\")\n",
        "\n",
        "    # Mostrar las clases de los 5 documentos más similares\n",
        "    for i in mostsim:\n",
        "        print(f\"Documento similar {i}:\")\n",
        "        print(f\"Clase: {newsgroups_train.target_names[y_train[i]]}\")\n",
        "        print(f\"Contenido (primeros 300 caracteres): {newsgroups_train.data[i][:100]}\\n\")\n",
        "\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación (f1-score macro) en el conjunto de datos de test.\n",
        "\n",
        " Considerar cambiar parámteros de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial y ComplementNB."
      ],
      "metadata": {
        "id": "5nSaUihILwI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamiento de modelos de clasificación Naïve Bayes\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#Entrenamiento del modelo Naïve Bayes Multinomial**\n",
        "clf_multinomial = MultinomialNB()\n",
        "clf_multinomial.fit(X_train, y_train)\n",
        "\n",
        "#Vectorización de los datos de prueba\n",
        "# Cargamos el dataset de test\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Transformamos los datos de prueba usando el vectorizador TF-IDF\n",
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "\n",
        "#Predicción y cálculo del F1-score para MultinomialNB\n",
        "y_pred_multinomial = clf_multinomial.predict(X_test)\n",
        "f1_score_multinomial = f1_score(newsgroups_test.target, y_pred_multinomial, average=\"macro\")\n",
        "print(f'F1-score (macro) MultinomialNB: {f1_score_multinomial}')\n",
        "\n",
        "#Entrenamiento del modelo Naïve Bayes Complement\n",
        "clf_complement = ComplementNB()\n",
        "clf_complement.fit(X_train, y_train)\n",
        "\n",
        "#Predicción y cálculo del F1-score para ComplementNB\n",
        "y_pred_complement = clf_complement.predict(X_test)\n",
        "f1_score_complement = f1_score(newsgroups_test.target, y_pred_complement, average=\"macro\")\n",
        "print(f'F1-score (macro) ComplementNB: {f1_score_complement}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afRATLz-L2sq",
        "outputId": "982d6fa8-a1f9-4f4a-8022-f5acd35c3dbd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score (macro) MultinomialNB: 0.5854345727938506\n",
            "F1-score (macro) ComplementNB: 0.692953349950875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# **3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
      ],
      "metadata": {
        "id": "JS1u_5kle7U8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Transponer la matriz documento-término\n",
        "X_train_transposed = X_train.T\n",
        "\n",
        "# Definir las 5 palabras de interés\n",
        "palabras_interes = ['health', 'activity', 'nutrition', 'balance', 'income']\n",
        "\n",
        "# Evaluar la similitud de las palabras de interés\n",
        "for palabra in palabras_interes:\n",
        "    if palabra in tfidfvect.vocabulary_:\n",
        "        idx = tfidfvect.vocabulary_[palabra]\n",
        "        idx2word = {v: k for k, v in tfidfvect.vocabulary_.items()}\n",
        "        palabra_vect = X_train_transposed[idx]\n",
        "        palabras_similares = cosine_similarity(palabra_vect.reshape(1, -1), X_train_transposed)[0]\n",
        "        idx_mas_similares = np.argsort(palabras_similares)[::-1][1:6]\n",
        "        print(f'Palabra elegida: {palabra}')\n",
        "        for idx in idx_mas_similares:\n",
        "            print(f'Palabra similar encontrada: {idx2word[idx]} (similaridad: {palabras_similares[idx]:.4f})')\n",
        "    else:\n",
        "        print(f'La palabra \"{palabra}\" no está en el vocabulario.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wilG3T1JSXOB",
        "outputId": "341c269a-c5ae-4191-f03a-a2b994014609"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabra elegida: health\n",
            "Palabra similar encontrada: ohip (similaridad: 0.3304)\n",
            "Palabra similar encontrada: provincial (similaridad: 0.2998)\n",
            "Palabra similar encontrada: care (similaridad: 0.2824)\n",
            "Palabra similar encontrada: traditionalists (similaridad: 0.2799)\n",
            "Palabra similar encontrada: fiscally (similaridad: 0.2799)\n",
            "Palabra elegida: activity\n",
            "Palabra similar encontrada: lifeguards (similaridad: 0.2819)\n",
            "Palabra similar encontrada: rollerblade (similaridad: 0.2819)\n",
            "Palabra similar encontrada: homily (similaridad: 0.2819)\n",
            "Palabra similar encontrada: rugby (similaridad: 0.2819)\n",
            "Palabra similar encontrada: seatbelts (similaridad: 0.2819)\n",
            "Palabra elegida: nutrition\n",
            "Palabra similar encontrada: harperperennial (similaridad: 0.7458)\n",
            "Palabra similar encontrada: 272007 (similaridad: 0.7458)\n",
            "Palabra similar encontrada: gershoff (similaridad: 0.7458)\n",
            "Palabra similar encontrada: barbecued (similaridad: 0.6254)\n",
            "Palabra similar encontrada: unrealistically (similaridad: 0.5813)\n",
            "Palabra elegida: balance\n",
            "Palabra similar encontrada: gruffness (similaridad: 0.4148)\n",
            "Palabra similar encontrada: shrinks (similaridad: 0.3538)\n",
            "Palabra similar encontrada: gramm (similaridad: 0.3538)\n",
            "Palabra similar encontrada: sham (similaridad: 0.3538)\n",
            "Palabra similar encontrada: chargeing (similaridad: 0.3538)\n",
            "Palabra elegida: income\n",
            "Palabra similar encontrada: cranny (similaridad: 0.5829)\n",
            "Palabra similar encontrada: nook (similaridad: 0.5829)\n",
            "Palabra similar encontrada: exigency (similaridad: 0.5819)\n",
            "Palabra similar encontrada: stagger (similaridad: 0.5802)\n",
            "Palabra similar encontrada: regimes (similaridad: 0.5270)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L9Dhyke5YKD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de Texto con 20 Newsgroups\n",
        "\n",
        "En este proyecto, se utilizó el conjunto de datos **20 Newsgroups**, donde se aplicó la **vectorización** de documentos para convertir texto en representaciones numéricas, eliminando elementos no relevantes como encabezados, pies de página y citas. Se generó un vocabulario de **101,631 palabras**.\n",
        "\n",
        "Para asegurar la reproducibilidad, se definió una **semilla aleatoria** y se seleccionaron 5 documentos aleatorios para medir la **similaridad** con el resto del conjunto mediante **similaridad coseno**. Se identificaron los **5 documentos más similares**, junto con sus respectivas clases.\n",
        "\n",
        "Luego, se entrenaron modelos de clasificación utilizando **Naïve Bayes**, evaluando su desempeño con el **F1-score macro** en el conjunto de prueba. Se compararon los modelos **MultinomialNB** y **ComplementNB**, obteniendo un F1-score de **0.5854** para el primero y **0.6930** para el segundo, indicando que **ComplementNB** tuvo un mejor desempeño.\n",
        "\n",
        "Finalmente, se transpuso la matriz documento-término para obtener una matriz término-documento. Se eligieron **5 palabras de interés** ('health', 'activity', 'nutrition', 'balance', 'income') para estudiar su **similaridad**, analizando las 5 palabras más similares a cada una.\n",
        "\n",
        "## Reflexiones y Aprendizajes\n",
        "\n",
        "Esta experiencia ha sido especialmente interesante para mí, ya que me ha permitido aplicar técnicas de procesamiento de lenguaje natural y explorar modelos de clasificación de texto. Los resultados obtenidos, especialmente el desempeño del modelo **ComplementNB**, subrayan la importancia de elegir el modelo adecuado para mejorar la precisión en las clasificaciones.\n",
        "\n",
        "Además, esta práctica ha reforzado mi interés por seguir aprendiendo en el campo del análisis de datos y el procesamiento de texto. Reconozco que hay mucho más por descubrir y perfeccionar en este ámbito, y estoy motivado para continuar explorando nuevas metodologías y herramientas que me permitan mejorar mis habilidades y conocimientos.\n"
      ],
      "metadata": {
        "id": "9In8iv5EYLqh"
      }
    }
  ]
}